# Security

AI safety, alignment, guardrails, and protection tools for securing AI applications.

## üîí Categories

### AI Guardrails
- **NeMo Guardrails** - Programmable guardrails for LLM applications
- **Guardrails AI** - Adding guardrails to LLM applications
- **Llama Guard** - Input/output safeguard model by Meta
- **Presidio** - Data protection and de-identification toolkit
- **LangKit** - Guardrails for LLM applications

### Prompt Security
- **Rebuff** - Prompt injection detection
- **Lakera Guard** - Real-time AI security platform
- **Prompt Armor** - Enterprise prompt injection protection
- **HiddenLayer** - AI security platform
- **Pillar Security** - AI application security

### Content Moderation
- **OpenAI Moderation API** - Content moderation for safe outputs
- **Azure Content Safety** - AI content moderation service
- **Hive Moderation** - Content moderation APIs
- **Sightengine** - Visual and text content moderation
- **Two Hat** - Community safety and content moderation

### Red Teaming & Evaluation
- **Garak** - LLM vulnerability scanner
- **PyRIT** - Python Risk Identification Toolkit by Microsoft
- **Adversarial Robustness Toolbox (ART)** - ML model security testing
- **Counterfit** - ML security testing CLI by Microsoft
- **Atlas** - Red teaming for AI systems

## üõ°Ô∏è Security Best Practices

1. **Input Validation** - Always validate and sanitize user inputs
2. **Output Filtering** - Filter LLM outputs for sensitive content
3. **Rate Limiting** - Implement rate limiting to prevent abuse
4. **Monitoring** - Monitor for suspicious patterns and attacks
5. **Regular Testing** - Continuously red-team your AI applications

---

*[‚Üê Back to Skills Master Index](../../docs/SKILL_MASTER_INDEX.md)*
